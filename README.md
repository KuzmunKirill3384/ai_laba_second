# Реализация Word2Vec для русской литературы

Этот проект реализует алгоритм Word2Vec (Skip-gram с Negative Sampling) для обучения векторных представлений слов на произведениях Льва Толстого. Включает загрузку данных, предобработку текста, обучение модели и оценку результатов.

## Особенности

- Загрузка текстов с Project Gutenberg
- Предобработка текста с использованием токенизации NLTK
- Реализация Word2Vec с нуля с настраиваемыми параметрами
- Возможности:
  - Обучение с разной размерностью векторов
  - Негативное сэмплирование
  - Визуализация потерь
  - Поиск похожих слов
  - Решение аналогий (например, "мужчина относится к женщине как король к ?")

## Структура проекта

├── text_download.py # Загрузка текстов с Project Gutenberg
|
├── text_preprocessing.py # Предобработка текста (токенизация)
|
├── Word2Vec.py # Основная реализация Word2Vec
|
├── main.py # Обучение модели
|
├── word2vec_model_d100.py # Пример использования обученной модели
|
├── requirements.txt # Зависимости Python
|
└── install_dependencies.py # Скрипт установки зависимостей

## Использование

1. **Установите зависимости**:
   ```bash
   python install_dependencies.py
   python text_download.py
   python text_preprocessing.py
   python main.py
   python word2vec_model_d100.py
